{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/grad-min/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoModel\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 2610\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at neulab/codebert-c and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = 'neulab/codebert-c'\n",
    "#model_ckpt = 'neulab/codebert-cpp'\n",
    "#model_ckpt = 'Salesforce/codet5p-110m-embedding'\n",
    "#model_ckpt = 'microsoft/unixcoder-base'\n",
    "#model_ckpt = 'codesage/codesage-small'\n",
    "#model_ckpt = 'FacebookAI/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the file path here\n",
    "file_path = 'full_data.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_regex = r'(//[^\\n]*|\\/\\*[\\s\\S]*?\\*\\/)'\n",
    "newline_regex = '\\n{1,}'\n",
    "whitespace_regex = '\\s{2,}'\n",
    "\n",
    "def data_cleaning(inp, pat, rep):\n",
    "  return re.sub(pat, rep, inp)\n",
    "\n",
    "data['truncated_code'] = (data['code'].apply(data_cleaning, args=(comment_regex, ''))\n",
    "                                      .apply(data_cleaning, args=(newline_regex, ' '))\n",
    "                                      .apply(data_cleaning, args=(whitespace_regex, ' '))\n",
    "                         )\n",
    "# remove all data points that have more than 15000 characters\n",
    "length_check = np.array([len(x) for x in data['truncated_code']]) > 15000\n",
    "data = data[~length_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test_valid, y_train, y_test_valid = train_test_split(data.loc[:, data.columns != 'label'],\n",
    "                                                                data['label'],\n",
    "                                                                train_size=0.8,\n",
    "                                                                stratify=data['label']\n",
    "                                                               )\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test_valid.loc[:, X_test_valid.columns != 'label'],\n",
    "                                                    y_test_valid,\n",
    "                                                    test_size=0.5,\n",
    "                                                    stratify=y_test_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = X_train\n",
    "data_train['label'] = y_train\n",
    "data_test = X_test\n",
    "data_test['label'] = y_test\n",
    "data_valid = X_valid\n",
    "data_valid['label'] = y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "#dts = Dataset.from_pandas(data)\n",
    "dts = DatasetDict()\n",
    "dts['train'] = Dataset.from_pandas(data_train)\n",
    "dts['test'] = Dataset.from_pandas(pd.concat([data_test, data_valid]))\n",
    "dts['valid'] = Dataset.from_pandas(pd.concat([data_test, data_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/21760 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1768 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 21760/21760 [00:16<00:00, 1314.31 examples/s]\n",
      "Map: 100%|██████████| 5440/5440 [00:04<00:00, 1279.33 examples/s]\n",
      "Map: 100%|██████████| 5440/5440 [00:04<00:00, 1279.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_func(examples):\n",
    "      result = tokenizer(examples['truncated_code'])\n",
    "      return result\n",
    "\n",
    "dts = dts.map(tokenizer_func, batched=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neulab/codebert-c'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts.set_format('torch')\n",
    "dts.rename_column('label', 'labels')\n",
    "dts = dts.remove_columns(['code', 'truncated_code', '__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout=0.1, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # define embedding layers for encoding positions\n",
    "        self.pos_encoding = nn.Embedding(max_len, d_model, padding_idx=padding_idx)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        chunk_size, B, d_model = x.shape\n",
    "        position_ids = torch.arange(0, chunk_size, dtype=torch.int).unsqueeze(1).to(device)\n",
    "        position_enc = self.pos_encoding(position_ids) # (chunk_size, 1, d_model)\n",
    "        position_enc = position_enc.expand(chunk_size, B, d_model)\n",
    "        \n",
    "        # Add positional encoding to the input token embeddings\n",
    "        x = x + position_enc\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class CodeBertModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 max_seq_length: int = 512, \n",
    "                 chunk_size: int = 512, \n",
    "                 n_attn_head: int = 2,\n",
    "                 dim_feedforward: int = 768,\n",
    "                 padding_idx: int = 0,\n",
    "                 model_ckpt: str = 'Salesforce/codet5p-110m-embedding'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.embedding_model = AutoModel.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "        \n",
    "        dict_config = self.embedding_model.config.to_dict()\n",
    "        for sym in ['hidden_dim', 'embed_dim', 'hidden_size']:\n",
    "            if sym in dict_config.keys():\n",
    "                embed_dim = dict_config[sym]\n",
    "                \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim,\n",
    "                                                   nhead=n_attn_head,\n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   batch_first=False)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layer,\n",
    "                                                         num_layers=2,\n",
    "                                                         )\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(max_len=max_seq_length, \n",
    "                                                      d_model=embed_dim, \n",
    "                                                      padding_idx=padding_idx)\n",
    "        \n",
    "        self.loss_func = nn.CrossEntropyLoss(weight=torch.Tensor([1.0, 3.0]),\n",
    "                                             label_smoothing=0.2)\n",
    "        \n",
    "        self.ffn = nn.Sequential(nn.Dropout(p=0.1),\n",
    "                                 nn.Linear(embed_dim, 2)\n",
    "                                 )\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def prepare_chunk(self, input_ids: torch.Tensor, \n",
    "                            attention_mask: torch.Tensor,\n",
    "                            labels=None):\n",
    "        \"\"\"\n",
    "        Prepare inputs into chunks that self.embedding_model can process (length < context_length)\n",
    "        Shape info:\n",
    "        - input_ids: (B, L)\n",
    "        - attention_mask: (B, L)\n",
    "        \"\"\"\n",
    "        \n",
    "        device = input_ids.device\n",
    "        # calculate number of chunks\n",
    "        num_chunk = input_ids.shape[-1] // self.chunk_size\n",
    "        if input_ids.shape[-1] % self.chunk_size != 0:\n",
    "            num_chunk += 1\n",
    "            pad_len = self.chunk_size - (input_ids.shape[-1] % self.chunk_size)\n",
    "        else: \n",
    "            pad_len = 0\n",
    "        \n",
    "        B = input_ids.shape[0]\n",
    "        # get the model's pad_token_id\n",
    "        pad_token_id = self.embedding_model.config.pad_token_id\n",
    "        \n",
    "        # create a pad & zero tensor, then append it to the input_ids & attention_mask tensor respectively\n",
    "        pad_tensor = torch.Tensor([pad_token_id]).expand(input_ids.shape[0], pad_len).int().to(device)\n",
    "        zero_tensor = torch.zeros(input_ids.shape[0], pad_len).int().to(device)\n",
    "        padded_input_ids = torch.cat([input_ids, pad_tensor], dim = -1).T # (chunk_size * num_chunk, B)\n",
    "        padded_attention_mask = torch.cat([attention_mask, zero_tensor], dim = -1).T # (chunk_size * num_chunk, B)\n",
    "                                                         \n",
    "        chunked_input_ids = padded_input_ids.reshape(num_chunk, self.chunk_size, B).permute(0, 2, 1) # (num_chunk, B, chunk_size)\n",
    "        chunked_attention_mask = padded_attention_mask.reshape(num_chunk, self.chunk_size, B).permute(0, 2, 1) # (num_chunk, B, chunk_size)\n",
    "        \n",
    "        pad_chunk_mask = self.create_chunk_key_padding_mask(chunked_input_ids)\n",
    "        \n",
    "        return chunked_input_ids, chunked_attention_mask, pad_chunk_mask\n",
    "    \n",
    "    def create_chunk_key_padding_mask(self, chunks):\n",
    "        \"\"\"\n",
    "        If a chunk contains only pad tokens, ignore that chunk\n",
    "        chunks: B, num_chunk, chunk_size\n",
    "        \"\"\"\n",
    "        pad_token_id = self.embedding_model.config.pad_token_id\n",
    "        pad_mask = (chunks == pad_token_id)\n",
    "        \n",
    "        num_pad = (torch.sum(pad_mask, -1) == self.chunk_size).permute(1, 0) # (num_chunk, B)\n",
    "        \n",
    "        return num_pad\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \n",
    "        # calculate numbers of chunk\n",
    "        chunked_input_ids, chunked_attention_mask, pad_chunk_mask = self.prepare_chunk(input_ids, attention_mask) # (num_chunk, B, chunk_size), (num_chunk, B, chunk_size), (num_chunk, B)\n",
    "        \n",
    "        # reshape input_ids & attention_mask tensors to fit into embedding model\n",
    "        num_chunk, B, chunk_size = chunked_input_ids.shape\n",
    "        chunked_input_ids, chunked_attention_mask = chunked_input_ids.contiguous().view(-1, chunk_size), chunked_attention_mask.contiguous().view(-1, self.chunk_size) # (B * num_chunk, chunk_size), (B * num_chunk, chunk_size)\n",
    "        \n",
    "        # roberta-exclusive\n",
    "        embedded_chunks = (self.embedding_model(input_ids = chunked_input_ids,\n",
    "                                                attention_mask = chunked_attention_mask)['pooler_output'] # (B * num_chunk, self.embedding_model.config.hidden_dim)\n",
    "                               .view(num_chunk, B, -1) # (num_chunk, B, self.embedding_model.config.hidden_dim)\n",
    "                          )\n",
    "        \n",
    "        # embedded_chunks = (self.embedding_model(input_ids = chunked_input_ids,\n",
    "        #                                         attention_mask = chunked_attention_mask) # (B * num_chunk, self.embedding_model.config.hidden_dim)\n",
    "        #                        .view(num_chunk, B, -1) # (num_chunk, B, self.embedding_model.config.hidden_dim)\n",
    "        #                   )\n",
    "        \n",
    "        # embedded_chunks = self.positional_encoding(embedded_chunks)\n",
    "        \n",
    "        # output = self.transformer_encoder(embedded_chunks, \n",
    "        #                                   src_key_padding_mask = pad_chunk_mask) # (num_chunk, B, self.embedding_model.config.hidden_dim)\n",
    "        \n",
    "        # logits = self.ffn(output[0])\n",
    "        chunk_mean = torch.mean(embedded_chunks, 0)\n",
    "        logits = self.ffn(chunk_mean)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_func(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        \n",
    "        return {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = np.argmax(eval_pred.predictions, -1), eval_pred.label_ids\n",
    "    return {'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred),\n",
    "            'recall': recall_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at neulab/codebert-c and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/grad-min/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = CodeBertModel(model_ckpt=model_ckpt,\n",
    "                      n_attn_head=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grad-min/.local/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_arguments = TrainingArguments(output_dir = 'my_codeberta',\n",
    "                                      evaluation_strategy = 'epoch',\n",
    "                                      per_device_train_batch_size = 2,\n",
    "                                      per_device_eval_batch_size = 2,\n",
    "                                      gradient_accumulation_steps = 12,\n",
    "                                      learning_rate = 2e-5,\n",
    "                                      num_train_epochs = 3,\n",
    "                                      warmup_ratio = 0.1,\n",
    "                                      lr_scheduler_type = 'cosine',\n",
    "                                      logging_strategy = 'steps',\n",
    "                                      logging_steps = 10,\n",
    "                                      save_strategy = 'no',\n",
    "                                      fp16 = True,\n",
    "                                      metric_for_best_model = 'recall',\n",
    "                                      optim = 'adamw_torch',\n",
    "                                      report_to = 'none'\n",
    "                                      )\n",
    "trainer = Trainer(model=model,\n",
    "                  data_collator=data_collator,\n",
    "                  args=training_arguments,\n",
    "                  train_dataset=dts['train'],\n",
    "                  eval_dataset=dts['valid'],\n",
    "                  compute_metrics=compute_metrics,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2718' max='2718' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2718/2718 23:19, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.647100</td>\n",
       "      <td>0.655965</td>\n",
       "      <td>0.509007</td>\n",
       "      <td>0.480703</td>\n",
       "      <td>0.959274</td>\n",
       "      <td>0.640463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.638768</td>\n",
       "      <td>0.577206</td>\n",
       "      <td>0.520747</td>\n",
       "      <td>0.910887</td>\n",
       "      <td>0.662658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.624400</td>\n",
       "      <td>0.640001</td>\n",
       "      <td>0.604963</td>\n",
       "      <td>0.541825</td>\n",
       "      <td>0.864516</td>\n",
       "      <td>0.666149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2718, training_loss=0.6486820251824138, metrics={'train_runtime': 1400.4906, 'train_samples_per_second': 46.612, 'train_steps_per_second': 1.941, 'total_flos': 0.0, 'train_loss': 0.6486820251824138, 'epoch': 2.997794117647059})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import wandb\n",
    "#wandb.init()\n",
    "trainer.train()\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = trainer.predict(dts['test'])\n",
    "y_true = dts['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'predictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      2\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'predictions'"
     ]
    }
   ],
   "source": [
    "y_pred = y_pred.predictions.argmax(-1).tolist()\n",
    "y_true = y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1246, 1714],\n",
       "       [ 310, 2170]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(conf, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 40.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in /home/grad-min/.local/lib/python3.8/site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/grad-min/.local/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/grad-min/.local/lib/python3.8/site-packages (from seaborn) (3.7.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/grad-min/.local/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/grad-min/.local/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/grad-min/.local/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/grad-min/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/grad-min/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/grad-min/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/grad-min/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/grad-min/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/grad-min/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/grad-min/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/grad-min/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2->seaborn) (1.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/grad-min/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib!=3.6.1,>=3.4->seaborn) (3.18.2)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
