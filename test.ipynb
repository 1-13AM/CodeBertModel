{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "seed = 2610\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = np.argmax(eval_pred.predictions, -1), eval_pred.label_ids\n",
    "    return {'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred),\n",
    "            'recall': recall_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred)}\n",
    "\n",
    "def data_cleaning(data, max_char=15000):\n",
    "    comment_regex = r'(//[^\\n]*|\\/\\*[\\s\\S]*?\\*\\/)'\n",
    "    newline_regex = '\\n{1,}'\n",
    "    whitespace_regex = '\\s{2,}'\n",
    "    def replace(inp, pat, rep):\n",
    "        return re.sub(pat, rep, inp)\n",
    "\n",
    "    data['truncated_code'] = (data['code'].apply(replace, args=(comment_regex, ''))\n",
    "                                        .apply(replace, args=(newline_regex, ' '))\n",
    "                                        .apply(replace, args=(whitespace_regex, ' '))\n",
    "                            )\n",
    "    data = data.sort_values(by='truncated_code', key=lambda x: x.str.len())\n",
    "    # remove all data points that have more than 15000 characters\n",
    "    length_check = np.array([len(x) for x in data['truncated_code']]) > max_char\n",
    "    data = data[~length_check]\n",
    "    return data\n",
    "def to_huggingface_dataset(data_train, data_test, data_valid):\n",
    "    dts = DatasetDict()\n",
    "    dts['train'] = Dataset.from_pandas(data_train)\n",
    "    dts['test'] = Dataset.from_pandas(pd.concat([data_test, data_valid]))\n",
    "    dts['valid'] = Dataset.from_pandas(pd.concat([data_test, data_valid]))\n",
    "    dts.set_format('torch')\n",
    "    dts.rename_column('label', 'labels')\n",
    "    dts = dts.remove_columns(['code', 'truncated_code', '__index_level_0__'])\n",
    "    \n",
    "    return dts\n",
    "\n",
    "def train_test_valid_split(data, train_size=0.8, test_size=0.1, valid_size=0.1):\n",
    "    X_train, X_test_valid, y_train, y_test_valid = train_test_split(data.loc[:, data.columns != 'label'],\n",
    "                                                                data['label'],\n",
    "                                                                train_size=train_size,\n",
    "                                                                stratify=data['label']\n",
    "                                                               )\n",
    "    test_size /= (test_size+valid_size)\n",
    "    X_test, X_valid, y_test, y_valid = train_test_split(X_test_valid.loc[:, X_test_valid.columns != 'label'],\n",
    "                                                        y_test_valid,\n",
    "                                                        test_size=test_size,\n",
    "                                                        stratify=y_test_valid)\n",
    "    data_train = X_train\n",
    "    data_train['label'] = y_train\n",
    "    data_test = X_test\n",
    "    data_test['label'] = y_test\n",
    "    data_valid = X_valid\n",
    "    data_valid['label'] = y_valid\n",
    "    print(data_train)\n",
    "    dts = to_huggingface_dataset(data_train, data_test, data_valid)\n",
    "    return dts\n",
    "\n",
    "def data_preprocessing(file_path=\"full_data.csv\",\n",
    "                       model_ckpt='neulab/codebert-c'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data_cleaning(data)\n",
    "    dts = train_test_valid_split(data)\n",
    "    \n",
    "    return dts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    code  \\\n",
      "9182   static size_t curl_size_cb(void *ptr, size_t s...   \n",
      "12554  static void *iothread_run(void *opaque)\\n\\n{\\n...   \n",
      "5908   static void v9fs_readlink(void *opaque)\\n\\n{\\n...   \n",
      "7554   static int qxl_init_secondary(PCIDevice *dev)\\...   \n",
      "9493   static int nprobe(AVFormatContext *s, uint8_t ...   \n",
      "...                                                  ...   \n",
      "3203   static av_cold void alloc_temp(HYuvContext *s)...   \n",
      "8208   static int commit_direntries(BDRVVVFATState* s...   \n",
      "21988  void avfilter_free(AVFilterContext *filter)\\n\\...   \n",
      "23635  static int old_codec47(SANMVideoContext *ctx, ...   \n",
      "1961   static void test_i440fx_defaults(gconstpointer...   \n",
      "\n",
      "                                          truncated_code  label  \n",
      "9182   static size_t curl_size_cb(void *ptr, size_t s...      1  \n",
      "12554  static void *iothread_run(void *opaque) { IOTh...      1  \n",
      "5908   static void v9fs_readlink(void *opaque) { V9fs...      0  \n",
      "7554   static int qxl_init_secondary(PCIDevice *dev) ...      1  \n",
      "9493   static int nprobe(AVFormatContext *s, uint8_t ...      0  \n",
      "...                                                  ...    ...  \n",
      "3203   static av_cold void alloc_temp(HYuvContext *s)...      1  \n",
      "8208   static int commit_direntries(BDRVVVFATState* s...      1  \n",
      "21988  void avfilter_free(AVFilterContext *filter) { ...      1  \n",
      "23635  static int old_codec47(SANMVideoContext *ctx, ...      1  \n",
      "1961   static void test_i440fx_defaults(gconstpointer...      1  \n",
      "\n",
      "[21760 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "out = data_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label'],\n",
       "        num_rows: 21760\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label'],\n",
       "        num_rows: 5440\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['label'],\n",
       "        num_rows: 5440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
